# Ollama Configuration
# Base URL for Ollama API (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Model to use for chat (default: llama3.2:1b)
# Examples: llama3.2:1b, llama3.2:3b, mistral, codellama
OLLAMA_MODEL=llama3.2:1b

# Server Configuration
PORT=3001

# Cors Configuration
CORS_ORIGINS=http://localhost:3000
